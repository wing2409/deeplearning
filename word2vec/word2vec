from konlpy.tag import Okt
from gensim.models import Word2Vec

okt=Okt()
fread = open('data/wiki_data.txt', encoding="utf8")
# 파일을 다시 처음부터 읽음.
n=0
result = []

while True:
    line = fread.readline() #한 줄씩 읽음.
    if not line: break # 모두 읽으면 while문 종료.
    n=n+1
    if n%5000==0: # 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인지 출력.
        print("%d번째 While문."%n)
    tokenlist = okt.pos(line, stem=True, norm=True) # 단어 토큰화
    temp=[]
    for word in tokenlist:
        if word[1] in ["Noun"]: # 명사일 때만
            temp.append((word[0])) # 해당 단어를 저장함

    if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만
      result.append(temp) # 결과에 저장
fread.close()

model = Word2Vec(result, size=100, window=5, min_count=5, workers=4, sg=0)

a=model.wv.most_similar("문학")
b=model.wv.most_similar("상수")
c=model.wv.most_similar("수학")

print('문학', a)
print('상수', b)
print('수학', c)

#문학 [('또한', 0.999906063079834), ('함', 0.9998987317085266), ('및', 0.9998906850814819), ('구성', 0.9998905658721924), ('위해', 0.9998904466629028), ('것', 0.9998892545700073), ('발표', 0.9998849034309387), ('기도', 0.9998815059661865), ('사람', 0.9998794794082642), ('안', 0.9998792409896851)]
#상수 [('중력', 0.9998627305030823), ('수학', 0.9998184442520142), ('이용', 0.9998183250427246), ('계산', 0.9998133182525635), ('물리', 0.9997991323471069), ('때문', 0.9997978806495667), ('예', 0.9997973442077637), ('정리', 0.9997880458831787), ('수', 0.9997867941856384), ('측정', 0.999783992767334)]
#수학 [('이', 0.999917209148407), ('포함', 0.9999098777770996), ('이용', 0.9999083876609802), ('가지', 0.9999067783355713), ('대한', 0.9999035596847534), ('설명', 0.9999022483825684), ('때문', 0.9999012351036072), ('여러', 0.9998971223831177), ('것', 0.9998968243598938), ('과정', 0.9998945593833923)]